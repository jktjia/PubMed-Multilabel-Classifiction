% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% Other packages
\usepackage{makecell} 
\usepackage{caption}
\usepackage{graphicx}
\usepackage{cuted}
\usepackage{subcaption}

\graphicspath{ {../plots/} }



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CS 6120: Comparing Models for Multilabel Classification of PubMed Article
	Abstracts}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Nithish Bhat \and Ashish Thomas \and Jamie Tjia \\ Northeastern
	University \\ Boston, MA, United States \\ \texttt{\{bhat.nithi, thomas.ash, tjia.j\}@northeastern.edu}
	\\ }

\begin{document}
\maketitle
\begin{abstract}
	This is a template for your project report. This template is used for submissions to the ACL conference. You can see the original files here (\url{https://github.com/acl-org/acl-style-files/tree/master/latex}) which have additional instructions on how to use this template.
\end{abstract}

\section{Introduction}

The growth of biomedical literature has created a need for automated methods to organize and
classify research articles effectively. Manually labeling articles with relevant Medical Subject
Headings (MeSH) is labor-intensive and time-consuming, making automated multi-label classification
a helpful tool for researchers and clinicians.

This project aims to use a transformer-based language model, specifically BERT, to perform
multi-label classification on biomedical research articles. Instead of traditional single-label
classification, we use multi-label classification, as articles are typically associated with
multiple MeSH tags.

Our goals are: (1) to train and evaluate a BERT model for accurately predicting MeSH root labels
and (2) to compare its performance against more conventional models, including logistic regression,
CNNs, and RNNs. By doing so, we hope to demonstrate the advantages of transformer-based models in
capturing contextual information and label correlations in biomedical texts, ultimately providing a
tool for organizing and analyzing the PubMed corpus.

\section{Background/Related Work}

Describe any background or related work

\section{Data}

\begin{figure*}
	[!htb]
	\centering
	\includegraphics[width=\textwidth]{label_freq.png}
	\captionof{figure}{Frequency of MeSH Root Labels}
	\label{fig:label_freq}
\end{figure*}

The baseline and BERT models were all trained and tested on the dataset
\href{https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification}
{PubMed MultiLabel Text Classification Dataset MeSH}. This dataset contains information on 50,000
articles from PubMed, including each article's title, abstract, and Medical Subject Heading (MeSH)
labels.

MeSH labels are arranged into a nested hierarchy with 16 root labels (see Table 1). Two
labels---Humanities [K] and Publication Characteristics [V]---have been excluded due to their
infrequency in the available data. The 14 included labels each appear in at least ten percent of
the sample articles (see Figure 1).

\begin{table*}
	[p]
	\centering
	\begin{tabular}{ c l }
		\Xhline{1.5pt}
		\bf Label & \bf Name                                                         \\[0.5ex]
		\hline
		\hline
		A         & Anatomy                                                          \\
		\hline
		B         & Organisms                                                        \\
		\hline
		C         & Diseases                                                         \\
		\hline
		D         & Chemicals and Drugs                                              \\
		\hline
		E         & Analytical, Diagnostic and Therapeutic Techniques, and Equipment \\
		\hline
		F         & Psychiatry and Psychology                                        \\
		\hline
		G         & Phenomena and Processes                                          \\
		\hline
		H         & Disciplines and Occupations                                      \\
		\hline
		I         & Anthropology, Education, Sociology, and Social Phenomena         \\
		\hline
		J         & Technology, Industry, and Agriculture                            \\
		\hline
		K         & Humanities                                                       \\
		\hline
		L         & Information Science                                              \\
		\hline
		M         & Named Groups                                                     \\
		\hline
		N         & Health Care                                                      \\
		\hline
		V         & Publication Characteristics                                      \\
		\hline
		Z         & Geographicals                                                    \\[1ex]
		\Xhline{1.5pt}
	\end{tabular}
	\captionof{table}{Full Label Names}
	\label{tab:label_names}
\end{table*}

Each article has up to 13 root labels, with a median root label count of six (see Figure 2). The
article abstracts are an average of 222 tokens long (see Figure 3).

\begin{figure*}
	[!htb]
	\includegraphics[width=\textwidth]{labels_per_sample.png}
	\captionof{figure}{Number of Labels per Abstract}
	\label{fig:label_per_sample}
\end{figure*}

\begin{figure*}
	[!htb]
	\includegraphics[width=\textwidth]{text_lengths.png}
	\captionof{figure}{Distribution of Abstract Lengths}
	\label{fig:text_len}
\end{figure*}

\section{Methods}

We use BioBERT---a specialized language model based on the dmis-lab/biobert-base-cased-v1.2---as a
starting point. While regular BERT learns from general texts such as Wikipedia, BioBERT trains on
vast biomedical data like PubMed summaries and PMC papers - so it understands medical meaning
better. Instead of "and," it relies on connections like "while" or "whereas." It handles split-up
words (limited to 128 pieces per summary) via 12 layered transformers. Rather than stacking ideas
with "plus," it builds step by step. From the last processing stage, the [CLS] marker's combined
signal feeds into a straight-line classifier that outputs scores across 14 units. Training adjusts
weights using BCEWithLogitsLoss - not only efficient but also suited for cases when one document
fits several MeSH labels at once. Each prediction passes through its own sigmoid function
separately; therefore, overlapping tags are possible without conflict.

The performance of the fine-tuned BERT model was measured against three baseline models which were
trained as follows.

The logistic regression model serves as a basic neural starting point while performing effectively
like a "Neural Bag-of-Words" system. Although it uses an embedding layer set to 100 dimensions by
default, this component converts word IDs into compact vector forms. Rather than tracking sequence
position, it computes the average of these embedded values throughout the full text entry. As a
result, one consolidated mean vector emerges, capturing the overall meaning of the abstract. This
average form feeds straight into a linear layer, then passes through a Sigmoid function. The
approach estimates each label's likelihood separately, yet maintains fast response times.

The CNN approach detects local word sequences and meaningful phrases in text, no matter where they
appear. Instead of focusing on exact positions, it processes embedded inputs using a 1D conv layer
with a broad window---size 25---and moves forward by steps of 10 to capture distinct signals from
the abstract. Following this step, global max pooling selects the strongest activation per filter
over the full input span. Then, these selected outputs pass through a Sigmoid function so that
multiple labels can be predicted at once.

The RNN model handles word order by applying a Bi-LSTM setup. First, an embedding step converts
split text into 256-element vectors. Then, these representations enter a Bi-LSTM block containing
128 memory cells. Instead of one direction only, processing occurs left-to-right and right-to-left
to grasp full-sentence meaning. Afterward, outputs from both flows merge before entering a dense
layer. That last component computes unnormalized scores directly. During inference, logits become
probabilities using a Sigmoid activation. With a 0.3 cutoff, multi-label outputs turn into binary
results.

\section{Experiments}

All of the baseline models were trained over 25 epochs, using an initial learning rate of 0.001
with Adam optimization.

At the end of each epoch, the micro F1, macro F1, weighted-average F1, exact match ratio, and
Hamming loss were calculated. Micro F1 calculates the F1 score using precision and recall over all
of the labels at once, while the macro F1 calculates the F1 for each label and returns the average
across the labels. Similarly to macro F1, weighted-average F1 calculates the F1 for each label and
returns the weighted average. The exact match ratio measures how many predictions exactly matched
every label, and Hamming loss measures how many labels were predicted incorrectly across all
labels.Higher F1 scores and exact match ratios indicate better performance, and lower Hamming loss
indicates better performance. Together, these five metrics were used to measure the performance of
each of the models.

\begin{figure*}[!htb]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{macro_f1.png}
		\captionof{figure}{Macro F1}
		\label{fig:macro_f1}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{micro_f1.png}
		\captionof{figure}{Micro F1}
		\label{fig:micro_f1}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{weighted_f1.png}
		\captionof{figure}{Weighted F1}
		\label{fig:weighted_f1}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{exact_match_ratio.png}
		\captionof{figure}{Exact Match Ratio}
		\label{fig:exact_match}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{hamming_loss.png}
		\captionof{figure}{Hamming Loss}
		\label{fig:hamming_loss}
	\end{subfigure}
	\captionof{figure}{Performance Metrics by Epoch}
	\label{fig:perf_metrics}
\end{figure*}

\subsection{Results}

The CNN model had by far the worst performance, with lower F1 scores, lower exact match ratios, and
higher Hamming loss.

The RNN model performed best of the three baseline models. It had the highest macro and
weighted-average F1 scores of the three baseline models over all 25 epochs, and performed best
with regard to micro F1, exact match ratio, and Hamming loss for most of training.

Although linear regression had worse performance than RNN across all five metrics for the first
several epochs of training, the model using linear regression had the marginally better performance
with regard to micro F1, exact match ratio, and Hamming loss past 20 epochs of training.

\section{Conclusions}

Present your discussion/conclusions.

% \bibliography{custom,anthology}
% \bibliographystyle{acl_natbib}
\end{document}
